# BLIP-2精读

### 核心动机与背景

- 随着视觉-语言模型（VLP）规模的扩大，端到端（End-to-end）的预训练成本变得极高，视觉领域和自然语言领域都已经有了非常强大的单模态预训练模型（如ViT和LLM，但是直接冻结LLM很难进行跨模态对齐，因为LLM在单模态训练中未见过图像数据。
- 目标：设计一种轻量级的模块，充当图像编码器和LLM之间的桥梁，既能减少计算成本，又能避免灾难性遗忘

### 核心架构之Q-Former

轻量级Transformer，是连接冻结图像编码器和冻结LLM的关键组件

输入：接受一组固定数量的可学习的Query embeddings

子模块：

- image transformer：与冻结的图像编码器交互，提取视觉特征
- text transformer： 既可以作为文本编码器，也可以作为文本解码器

信息瓶颈：Q-Former迫使查询向量提取与文本最相关的视觉信息，过滤无关信息，减轻LLM学习视觉-语言对齐的负担

### 两阶段预训练策略

![image-20260103181221369](C:\Users\dy\AppData\Roaming\Typora\typora-user-images\image-20260103181221369.png)

##### 第一阶段：视觉-语言表征学习

![image-20260103193307926](C:\Users\dy\AppData\Roaming\Typora\typora-user-images\image-20260103193307926.png)

目标：训练Q-Former从冻结的图像编码器中提取与文本相关的视觉特征

输入：32个可学习的查询向量 和 文本 Token

损失函数设计：

图像-文本对比学习 (ITC)：对齐图像和文本的表示，最大化互信息。为了防止信息泄漏，使用单模态自注意力掩码，即Query和Text互相看不见。逼它提取出能概括全图的特征

基于图像的文本生成 (ITG)： 训练 Q-Former 在给定图像条件下生成文本。使用多模态因果自注意力掩码（Multimodal Causal Mask），Query之间可以互见，但不能看Text；Text可以看所有Query和之前的Text 。逼它提取出能描述细节的特征

图像-文本匹配 (ITM)： 二分类任务，判断图文是否匹配。使用双向自注意力掩码（Bi-directional Mask），允许所有Query和Text互相可见 。逼它提取出能与文本严丝合缝对齐的特征

目的：学习32个查询向量

##### 第二阶段：视觉到语言生成学习 

![image-20260103193709313](C:\Users\dy\AppData\Roaming\Typora\typora-user-images\image-20260103193709313.png)

目标： 将 Q-Former 的输出对接到冻结的 LLM，利用 LLM 的生成能力 。 

输入：第一阶段得到的32个查询向量

- 因为32 个query的维度与LLM的输入维度不匹配，因此先通过一个FC层映射到相同维度，同时query也是必须进行微调的
- 针对不同的LLM因材施教：
  - 基于解码器的LLM：如OPT,输入 = `[32个视觉向量] + [文本]`,任务 = 预测下一个词
  - W基于编码器-解码器的 LLM：把文本拆成两半，**前缀 (Prefix)** 和 **后缀 (Suffix)** ，**输入：** [32个视觉向量] + [前缀文本]，用于训练生成后缀文本



### 局限性

- 信息过时：LLM冻结，不知道训练数据截止之后的新信息
- 主要是LLM的问题
- 上下文能力缺失