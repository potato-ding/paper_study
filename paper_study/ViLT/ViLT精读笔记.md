#  ViLT精读笔记

### 核心背景

传统模型依赖**目标检测器**（如 Faster R-CNN）来提取图像的“区域特征”（Region Features）。这个提取过程非常慢，甚至比后续的多模态交互步骤还要耗时得多

模型的视觉表达能力受限于预训练好的目标检测器及其固定的视觉词汇表，无法端到端地学习

### 解决思路

像处理文本一样处理图像

### 模型架构

##### Visual embedding:

传统做法： 使用深层 CNN处理图像，提取特征图或物体区域

ViLT做法：

- 将输入图像切成 $32 \times 32$ 的 Patch
- 将这些 Patch 展平，并通过一个简单的线性层映射到嵌入空间
- 这个过程仅需 2.4M 参数，而传统的 ResNet-101 需要约 44M 参数，甚至更多 

##### 模态交互

- 文本 Embedding 和 图像 Patch Embedding 被直接拼接在一起，输入到一个统一的 Transformer 编码器中（Single-Stream架构）
- 参数初始化：利用在 ImageNet 上预训练好的 **ViT-B/32** 进行初始化的，意味着模型天生就具备了处理 Patch 的能力

### 预训练

-  图像文本匹配ITM

模型通过一个单层的全连接层将 $p$ 映射为一个二分类的 logits，是一个粗粒度的对齐，教模型学会“看大概”，判断整体语义是否一致

- 掩码语言建模MLM

普通的 Tokenizer 会把单词切碎只mask部分字母，但是ViLT强制 Mask 掉属于同一个单词的所有 token，这样模型如果不看图像中的“长颈鹿”特征，就无法还原这个词

- 单词-Patch 对齐WPA：重要的创新点

为了弥补“没有物体检测框”而引入的最硬核的数学工具

困境：视觉特征是基于区域的，视觉输入是一个个明确的钨锑矿，对齐任务很容易计算词与框之间的相似度。ViLT没有框，所以只知道图中有什么，而不知道哪一部分是目标啊。因此WPA的任务是强迫把词的注意力运输到图中对应的几个patch上

核心原理：最优传输：找到一个最佳运输方案，将单词的语义质量分配到patch上，使总运输成本最低

算法实现细节：IPOT 算法

设计一个代价矩阵C：计算每个单词i和每个patch j之间的欧氏距离或者余弦距离

初始化一个运输方案矩阵F，迭代计算F * C并更新这个F，最后得到的这个矩阵F就作为单词和patch对齐的矩阵，通过这个最后得到的F所计算的loss才会进入反向传播

- MPP

把图像的 Patch 也 Mask 掉，让模型根据上下文预测被遮挡 Patch 的 **RGB 均值** ，这个做法没有作用，因此被放弃

### 实验结果

ViLT的主要贡献是对于推理速度的加强。证明了 Transformer 架构本身具有强大的跨模态处理能力，而不需要人为设计的视觉特征（物体检测框）