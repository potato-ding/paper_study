# Clip笔记

### 核心动机：

传统的计算机视觉系统通常训练用于预测一组预先定义的物体类别，这种方式限制了模型的通用性，因为遇到新概念时需要重新标注数据和训练

Clip:直接从关于图像的**原始文本**中学习，这是一种更广泛的监督来源

### 训练方法：对比预训练（Contrastive Pre-training）

- 训练数据：使用了从互联网收集的 **4 亿对**（图像，文本）数据集，称为 **WIT**
- **训练目标**：给定一个包含 $N$ 个图像-文本对的 Batch，CLIP 被训练去预测这 $N \times N$ 种可能的组合中，哪些才是真实的配对 6。
- **模型结构**：同时训练一个**图像编码器**和一个**文本编码器**，通过最大化真实配对的余弦相似度、最小化错误配对的相似度来实现

![image-20260103115827011](C:\Users\dy\AppData\Roaming\Typora\typora-user-images\image-20260103115827011.png)

### 核心痛点：

### 零样本迁移

在传统深度学习中，如果你训练了一个识别猫狗的模型，想让它识别“飞机”，你必须修改输出层（Softmax）并重新训练

- 构建分类器：Clip将每个类别填入模板“A photo of a {label}”，得到对应的文本，得到对应的text encoder
- 图像推理： 把待测图片输入image encoder得到图像向量，并计算与文本的特征相似度，得到相似度最高的预测结果
- 提示词工程（Prompt Engineering）：上面的模板明确表示是一个图片，解决多义词问题，甚至可以使用多个模板，生成特征取平均值效果更好
- 在使用图片分类时，完全冻结image encoder，只训练最后一层逻辑回归分类器

### 鲁棒性问题

分布偏移问题：在真实照片上训练好的RESNET，去识别卡通画或者素描画准确率会断崖式下跌，原因是网络记住了黄色的。弯曲的等特征而没有真正去学。

有效鲁棒性：模型在面对困难数据时比预期表现好了多少。Clip进行了语义层面的理解，真正解决泛化能力

### 缺点

- 细粒度分类：对于车、狗分类很强，但对于不同种类的兰花就很难，因为文本很少会对图片做细致的区分描述
- 抽象逻辑任务：像“图里这辆车离我多远”这样的问题无法回答
- CLIP 胜在**泛化能力**和**易用性**，而不是在单一任务上的绝对精度。如果你只需要解决一个非常具体的任务（比如人脸识别），专门的模型还是更好
- 数据效率极低
- 社会偏见：学到了互联网上的偏见