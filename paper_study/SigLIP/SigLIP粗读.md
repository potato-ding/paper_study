# SigLIP粗读

### CLIP 

对于每张照片（Image），CLIP 必须看遍全场所有的个人简介（Text），然后通过 Softmax 归一化，选出**唯一**最匹配的一个

**缺点：** 计算量巨大。为了算出谁是“全场最佳”，所有 GPU 必须频繁交换数据（Global Synchronization），这限制了你能处理的“相亲人数”（Batch Size）

### SigLIP

SigLIP 不关心“谁是全场最佳”。它拿起每一对照片和简介，只问一个简单的问题：**“这一对匹配吗？**

**优点：** 互不干扰。不需要知道别人的匹配分数，也不需要全场归一化。这使得训练效率极其高效，Batch Size 可以无限扩大

### 技术细节对比

在 CLIP 的标准对比学习（Contrastive Learning）中，我们使用的是 **Softmax**。Softmax 强制要求概率之和为 1，这意味着正样本（正确的图文对）和负样本（错误的图文对）是**互相依赖**的。

**SigLIP 的创新点：** 它将图像-文本预训练重新定义为**成对的二分类问题**

![image-20260103202847613](C:\Users\dy\AppData\Roaming\Typora\typora-user-images\image-20260103202847613.png)

### 对于物理智能

- 对“高分辨率”的渴求：机器人操作需要极高的视觉精度。传统的 CLIP 受到 Batch Size 限制，往往只能在较小的图片分辨率（如 224x224）下训练，否则显存会爆。 SigLIP 省内存且无需全局同步，Google 可以在**更高的分辨率**下，保持**巨大的 Batch Size** 进行训练。这让 $\pi_0$ 的视觉编码器能看清物理世界的细节。
- 更好的多模态对齐：$\pi_0$ 需要理解复杂的物理指令，SigLIP 的训练方式让模型对图像的局部特征和语言的细微差别更加敏感，这为 $\pi_0$ 处理复杂的 VLA (Vision-Language-Action) 任务提供了更坚实的感知基础
- NaViT 架构的完美搭档：sigLIP 允许模型处理任意长宽比的图片，不需要强制裁剪成正方形。对于机器人来说，视野原本就是宽屏的或不规则的，SigLIP + NaViT 让机器人看到了“未被裁剪的真实世界”