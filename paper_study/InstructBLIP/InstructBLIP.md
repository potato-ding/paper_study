# InstructBLIP

BLIP-2 虽然通过 Q-Former 很好地连接了视觉和语言模型，但它生成的视觉特征是“静态”的，即无论你问什么问题，提取的视觉特征都是一样的

###  架构改进：指令感知的视觉特征提取

- **BLIP-2 的局限：** 在 BLIP-2 中，Q-Former 的输入是一组固定的可学习 Query，它提取的视觉特征与用户的具体指令（Prompt）无关。这意味着，无论你问“这只狗是什么颜色的？”还是“图片里有几个人？”，送给 LLM 的视觉特征都是同一套 。

- **InstructBLIP 的改进：** InstructBLIP 将文本指令（Instruction）也作为输入送入 **Q-Former** 

  思考：BLIP-12中第一阶段的文本输入不影响图像的特征提取，因此会提取一个万金油式的通用视觉特征

  - **具体做法：** 指令文本不仅给到冻结的 LLM，同时也给到 Q-Former。通过 Q-Former 内部的 Self-Attention 层，指令会与 Query Embeddings 进行交互 。也就是说除了32个query向量还有文本token的拼接共同训练Q-Former。
  - **效果：** 这使得 Q-Former 能够根据指令提取“定制化”的视觉特征。例如，如果指令是关于位置的，Q-Former 就会更关注空间信息；如果是关于颜色的，就更关注色彩信息 。实验证明，去掉这个机制会导致性能显著下降 。

### 数据改进

- **数据重组：** InstructBLIP 收集了 **26 个公开数据集**，涵盖了图像描述、视觉推理、视觉问答（VQA）、对话等 11 类任务 。
- **格式转化：** 它将这些数据集全部转化为“指令-响应”（Instruction-Response）的格式。对于每个任务，作者精心设计了 10-15 个不同的自然语言指令模板 。
- **训练策略：** 即使没见过的数据集（Held-out），InstructBLIP 也能表现出色，因为它学习的是“如何遵循指令”而非死记硬背特定任务 。

### 训练策略改进：平衡采样

提出了一种按数据集大小平方根比例进行采样的策略 